{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac64b6a2-b5be-4be7-aa5e-5a1553eb30c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\PUSHKAR\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\PUSHKAR\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\PUSHKAR\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\PUSHKAR\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7203cfe1-63db-4528-b634-6c0eacfa612b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_document = \"\"\"Text analytics involves analyzing unstructured text data to extract meaningful insights. \n",
    "It includes preprocessing steps such as tokenization, POS tagging, stop words removal, \n",
    "stemming, and lemmatization. Text analytics techniques are widely used in natural language \n",
    "processing (NLP), sentiment analysis, information retrieval, and text classification. \n",
    "The goal of text analytics is to transform text data into a structured format that can be \n",
    "used for further analysis and modeling. This process typically involves cleaning and \n",
    "preprocessing the text data, extracting features, and applying machine learning algorithms. \n",
    "Some common text analytics tasks include document classification, topic modeling, named \n",
    "entity recognition, and text summarization. With the increasing availability of textual \n",
    "data from sources such as social media, websites, and documents, text analytics has become \n",
    "an essential tool for businesses, researchers, and data scientists.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2099bd6a-ecc6-40c7-9815-e48a451e0f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Text',\n",
       " 'analytics',\n",
       " 'involves',\n",
       " 'analyzing',\n",
       " 'unstructured',\n",
       " 'text',\n",
       " 'data',\n",
       " 'to',\n",
       " 'extract',\n",
       " 'meaningful',\n",
       " 'insights',\n",
       " '.',\n",
       " 'It',\n",
       " 'includes',\n",
       " 'preprocessing',\n",
       " 'steps',\n",
       " 'such',\n",
       " 'as',\n",
       " 'tokenization',\n",
       " ',',\n",
       " 'POS',\n",
       " 'tagging',\n",
       " ',',\n",
       " 'stop',\n",
       " 'words',\n",
       " 'removal',\n",
       " ',',\n",
       " 'stemming',\n",
       " ',',\n",
       " 'and',\n",
       " 'lemmatization',\n",
       " '.',\n",
       " 'Text',\n",
       " 'analytics',\n",
       " 'techniques',\n",
       " 'are',\n",
       " 'widely',\n",
       " 'used',\n",
       " 'in',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " '(',\n",
       " 'NLP',\n",
       " ')',\n",
       " ',',\n",
       " 'sentiment',\n",
       " 'analysis',\n",
       " ',',\n",
       " 'information',\n",
       " 'retrieval',\n",
       " ',',\n",
       " 'and',\n",
       " 'text',\n",
       " 'classification',\n",
       " '.',\n",
       " 'The',\n",
       " 'goal',\n",
       " 'of',\n",
       " 'text',\n",
       " 'analytics',\n",
       " 'is',\n",
       " 'to',\n",
       " 'transform',\n",
       " 'text',\n",
       " 'data',\n",
       " 'into',\n",
       " 'a',\n",
       " 'structured',\n",
       " 'format',\n",
       " 'that',\n",
       " 'can',\n",
       " 'be',\n",
       " 'used',\n",
       " 'for',\n",
       " 'further',\n",
       " 'analysis',\n",
       " 'and',\n",
       " 'modeling',\n",
       " '.',\n",
       " 'This',\n",
       " 'process',\n",
       " 'typically',\n",
       " 'involves',\n",
       " 'cleaning',\n",
       " 'and',\n",
       " 'preprocessing',\n",
       " 'the',\n",
       " 'text',\n",
       " 'data',\n",
       " ',',\n",
       " 'extracting',\n",
       " 'features',\n",
       " ',',\n",
       " 'and',\n",
       " 'applying',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'algorithms',\n",
       " '.',\n",
       " 'Some',\n",
       " 'common',\n",
       " 'text',\n",
       " 'analytics',\n",
       " 'tasks',\n",
       " 'include',\n",
       " 'document',\n",
       " 'classification',\n",
       " ',',\n",
       " 'topic',\n",
       " 'modeling',\n",
       " ',',\n",
       " 'named',\n",
       " 'entity',\n",
       " 'recognition',\n",
       " ',',\n",
       " 'and',\n",
       " 'text',\n",
       " 'summarization',\n",
       " '.',\n",
       " 'With',\n",
       " 'the',\n",
       " 'increasing',\n",
       " 'availability',\n",
       " 'of',\n",
       " 'textual',\n",
       " 'data',\n",
       " 'from',\n",
       " 'sources',\n",
       " 'such',\n",
       " 'as',\n",
       " 'social',\n",
       " 'media',\n",
       " ',',\n",
       " 'websites',\n",
       " ',',\n",
       " 'and',\n",
       " 'documents',\n",
       " ',',\n",
       " 'text',\n",
       " 'analytics',\n",
       " 'has',\n",
       " 'become',\n",
       " 'an',\n",
       " 'essential',\n",
       " 'tool',\n",
       " 'for',\n",
       " 'businesses',\n",
       " ',',\n",
       " 'researchers',\n",
       " ',',\n",
       " 'and',\n",
       " 'data',\n",
       " 'scientists',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(sample_document)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "332e040f-1258-4152-bf52-32b274536d96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Text', 'JJ'),\n",
       " ('analytics', 'NNS'),\n",
       " ('involves', 'VBZ'),\n",
       " ('analyzing', 'VBG'),\n",
       " ('unstructured', 'JJ'),\n",
       " ('text', 'NN'),\n",
       " ('data', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('extract', 'VB'),\n",
       " ('meaningful', 'JJ'),\n",
       " ('insights', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('It', 'PRP'),\n",
       " ('includes', 'VBZ'),\n",
       " ('preprocessing', 'VBG'),\n",
       " ('steps', 'NNS'),\n",
       " ('such', 'JJ'),\n",
       " ('as', 'IN'),\n",
       " ('tokenization', 'NN'),\n",
       " (',', ','),\n",
       " ('POS', 'NNP'),\n",
       " ('tagging', 'NN'),\n",
       " (',', ','),\n",
       " ('stop', 'VB'),\n",
       " ('words', 'NNS'),\n",
       " ('removal', 'JJ'),\n",
       " (',', ','),\n",
       " ('stemming', 'VBG'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('lemmatization', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Text', 'NNP'),\n",
       " ('analytics', 'NNS'),\n",
       " ('techniques', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('widely', 'RB'),\n",
       " ('used', 'VBN'),\n",
       " ('in', 'IN'),\n",
       " ('natural', 'JJ'),\n",
       " ('language', 'NN'),\n",
       " ('processing', 'NN'),\n",
       " ('(', '('),\n",
       " ('NLP', 'NNP'),\n",
       " (')', ')'),\n",
       " (',', ','),\n",
       " ('sentiment', 'JJ'),\n",
       " ('analysis', 'NN'),\n",
       " (',', ','),\n",
       " ('information', 'NN'),\n",
       " ('retrieval', 'NN'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('text', 'JJ'),\n",
       " ('classification', 'NN'),\n",
       " ('.', '.'),\n",
       " ('The', 'DT'),\n",
       " ('goal', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('text', 'JJ'),\n",
       " ('analytics', 'NNS'),\n",
       " ('is', 'VBZ'),\n",
       " ('to', 'TO'),\n",
       " ('transform', 'VB'),\n",
       " ('text', 'NN'),\n",
       " ('data', 'NNS'),\n",
       " ('into', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('structured', 'JJ'),\n",
       " ('format', 'NN'),\n",
       " ('that', 'WDT'),\n",
       " ('can', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('used', 'VBN'),\n",
       " ('for', 'IN'),\n",
       " ('further', 'JJ'),\n",
       " ('analysis', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('modeling', 'NN'),\n",
       " ('.', '.'),\n",
       " ('This', 'DT'),\n",
       " ('process', 'NN'),\n",
       " ('typically', 'RB'),\n",
       " ('involves', 'VBZ'),\n",
       " ('cleaning', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('preprocessing', 'VBG'),\n",
       " ('the', 'DT'),\n",
       " ('text', 'NN'),\n",
       " ('data', 'NNS'),\n",
       " (',', ','),\n",
       " ('extracting', 'VBG'),\n",
       " ('features', 'NNS'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('applying', 'VBG'),\n",
       " ('machine', 'NN'),\n",
       " ('learning', 'NN'),\n",
       " ('algorithms', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Some', 'DT'),\n",
       " ('common', 'JJ'),\n",
       " ('text', 'NN'),\n",
       " ('analytics', 'NNS'),\n",
       " ('tasks', 'NNS'),\n",
       " ('include', 'VBP'),\n",
       " ('document', 'JJ'),\n",
       " ('classification', 'NN'),\n",
       " (',', ','),\n",
       " ('topic', 'NN'),\n",
       " ('modeling', 'NN'),\n",
       " (',', ','),\n",
       " ('named', 'VBN'),\n",
       " ('entity', 'NN'),\n",
       " ('recognition', 'NN'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('text', 'JJ'),\n",
       " ('summarization', 'NN'),\n",
       " ('.', '.'),\n",
       " ('With', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('increasing', 'VBG'),\n",
       " ('availability', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('textual', 'JJ'),\n",
       " ('data', 'NNS'),\n",
       " ('from', 'IN'),\n",
       " ('sources', 'NNS'),\n",
       " ('such', 'JJ'),\n",
       " ('as', 'IN'),\n",
       " ('social', 'JJ'),\n",
       " ('media', 'NNS'),\n",
       " (',', ','),\n",
       " ('websites', 'NNS'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('documents', 'NNS'),\n",
       " (',', ','),\n",
       " ('text', 'NN'),\n",
       " ('analytics', 'NNS'),\n",
       " ('has', 'VBZ'),\n",
       " ('become', 'VBN'),\n",
       " ('an', 'DT'),\n",
       " ('essential', 'JJ'),\n",
       " ('tool', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('businesses', 'NNS'),\n",
       " (',', ','),\n",
       " ('researchers', 'NNS'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('data', 'NNS'),\n",
       " ('scientists', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posTagWords = pos_tag(tokens)\n",
    "posTagWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d395400d-b83a-4b58-b437-d547761cc4ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Text',\n",
       " 'analytics',\n",
       " 'involves',\n",
       " 'analyzing',\n",
       " 'unstructured',\n",
       " 'text',\n",
       " 'data',\n",
       " 'extract',\n",
       " 'meaningful',\n",
       " 'insights',\n",
       " '.',\n",
       " 'includes',\n",
       " 'preprocessing',\n",
       " 'steps',\n",
       " 'tokenization',\n",
       " ',',\n",
       " 'POS',\n",
       " 'tagging',\n",
       " ',',\n",
       " 'stop',\n",
       " 'words',\n",
       " 'removal',\n",
       " ',',\n",
       " 'stemming',\n",
       " ',',\n",
       " 'lemmatization',\n",
       " '.',\n",
       " 'Text',\n",
       " 'analytics',\n",
       " 'techniques',\n",
       " 'widely',\n",
       " 'used',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " '(',\n",
       " 'NLP',\n",
       " ')',\n",
       " ',',\n",
       " 'sentiment',\n",
       " 'analysis',\n",
       " ',',\n",
       " 'information',\n",
       " 'retrieval',\n",
       " ',',\n",
       " 'text',\n",
       " 'classification',\n",
       " '.',\n",
       " 'goal',\n",
       " 'text',\n",
       " 'analytics',\n",
       " 'transform',\n",
       " 'text',\n",
       " 'data',\n",
       " 'structured',\n",
       " 'format',\n",
       " 'used',\n",
       " 'analysis',\n",
       " 'modeling',\n",
       " '.',\n",
       " 'process',\n",
       " 'typically',\n",
       " 'involves',\n",
       " 'cleaning',\n",
       " 'preprocessing',\n",
       " 'text',\n",
       " 'data',\n",
       " ',',\n",
       " 'extracting',\n",
       " 'features',\n",
       " ',',\n",
       " 'applying',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'algorithms',\n",
       " '.',\n",
       " 'common',\n",
       " 'text',\n",
       " 'analytics',\n",
       " 'tasks',\n",
       " 'include',\n",
       " 'document',\n",
       " 'classification',\n",
       " ',',\n",
       " 'topic',\n",
       " 'modeling',\n",
       " ',',\n",
       " 'named',\n",
       " 'entity',\n",
       " 'recognition',\n",
       " ',',\n",
       " 'text',\n",
       " 'summarization',\n",
       " '.',\n",
       " 'increasing',\n",
       " 'availability',\n",
       " 'textual',\n",
       " 'data',\n",
       " 'sources',\n",
       " 'social',\n",
       " 'media',\n",
       " ',',\n",
       " 'websites',\n",
       " ',',\n",
       " 'documents',\n",
       " ',',\n",
       " 'text',\n",
       " 'analytics',\n",
       " 'become',\n",
       " 'essential',\n",
       " 'tool',\n",
       " 'businesses',\n",
       " ',',\n",
       " 'researchers',\n",
       " ',',\n",
       " 'data',\n",
       " 'scientists',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "tokenized_words = [word for word in tokens if word.lower() not in stop_words]\n",
    "tokenized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "051f178c-f048-447a-979b-246b68dedae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "902dd362-44de-4a08-a358-414e17198378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text',\n",
       " 'analyt',\n",
       " 'involv',\n",
       " 'analyz',\n",
       " 'unstructur',\n",
       " 'text',\n",
       " 'data',\n",
       " 'extract',\n",
       " 'meaning',\n",
       " 'insight',\n",
       " '.',\n",
       " 'includ',\n",
       " 'preprocess',\n",
       " 'step',\n",
       " 'token',\n",
       " ',',\n",
       " 'po',\n",
       " 'tag',\n",
       " ',',\n",
       " 'stop',\n",
       " 'word',\n",
       " 'remov',\n",
       " ',',\n",
       " 'stem',\n",
       " ',',\n",
       " 'lemmat',\n",
       " '.',\n",
       " 'text',\n",
       " 'analyt',\n",
       " 'techniqu',\n",
       " 'wide',\n",
       " 'use',\n",
       " 'natur',\n",
       " 'languag',\n",
       " 'process',\n",
       " '(',\n",
       " 'nlp',\n",
       " ')',\n",
       " ',',\n",
       " 'sentiment',\n",
       " 'analysi',\n",
       " ',',\n",
       " 'inform',\n",
       " 'retriev',\n",
       " ',',\n",
       " 'text',\n",
       " 'classif',\n",
       " '.',\n",
       " 'goal',\n",
       " 'text',\n",
       " 'analyt',\n",
       " 'transform',\n",
       " 'text',\n",
       " 'data',\n",
       " 'structur',\n",
       " 'format',\n",
       " 'use',\n",
       " 'analysi',\n",
       " 'model',\n",
       " '.',\n",
       " 'process',\n",
       " 'typic',\n",
       " 'involv',\n",
       " 'clean',\n",
       " 'preprocess',\n",
       " 'text',\n",
       " 'data',\n",
       " ',',\n",
       " 'extract',\n",
       " 'featur',\n",
       " ',',\n",
       " 'appli',\n",
       " 'machin',\n",
       " 'learn',\n",
       " 'algorithm',\n",
       " '.',\n",
       " 'common',\n",
       " 'text',\n",
       " 'analyt',\n",
       " 'task',\n",
       " 'includ',\n",
       " 'document',\n",
       " 'classif',\n",
       " ',',\n",
       " 'topic',\n",
       " 'model',\n",
       " ',',\n",
       " 'name',\n",
       " 'entiti',\n",
       " 'recognit',\n",
       " ',',\n",
       " 'text',\n",
       " 'summar',\n",
       " '.',\n",
       " 'increas',\n",
       " 'avail',\n",
       " 'textual',\n",
       " 'data',\n",
       " 'sourc',\n",
       " 'social',\n",
       " 'media',\n",
       " ',',\n",
       " 'websit',\n",
       " ',',\n",
       " 'document',\n",
       " ',',\n",
       " 'text',\n",
       " 'analyt',\n",
       " 'becom',\n",
       " 'essenti',\n",
       " 'tool',\n",
       " 'busi',\n",
       " ',',\n",
       " 'research',\n",
       " ',',\n",
       " 'data',\n",
       " 'scientist',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_words = [stemmer.stem(word) for word in tokenized_words]\n",
    "stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1fa786f-9775-49ea-81bc-336571af63fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ae502d8-8ac6-405f-a451-ce7ddeac5d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Text',\n",
       " 'analytics',\n",
       " 'involve',\n",
       " 'analyze',\n",
       " 'unstructured',\n",
       " 'text',\n",
       " 'data',\n",
       " 'extract',\n",
       " 'meaningful',\n",
       " 'insight',\n",
       " '.',\n",
       " 'include',\n",
       " 'preprocessing',\n",
       " 'step',\n",
       " 'tokenization',\n",
       " ',',\n",
       " 'POS',\n",
       " 'tag',\n",
       " ',',\n",
       " 'stop',\n",
       " 'word',\n",
       " 'removal',\n",
       " ',',\n",
       " 'stem',\n",
       " ',',\n",
       " 'lemmatization',\n",
       " '.',\n",
       " 'Text',\n",
       " 'analytics',\n",
       " 'technique',\n",
       " 'widely',\n",
       " 'use',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'process',\n",
       " '(',\n",
       " 'NLP',\n",
       " ')',\n",
       " ',',\n",
       " 'sentiment',\n",
       " 'analysis',\n",
       " ',',\n",
       " 'information',\n",
       " 'retrieval',\n",
       " ',',\n",
       " 'text',\n",
       " 'classification',\n",
       " '.',\n",
       " 'goal',\n",
       " 'text',\n",
       " 'analytics',\n",
       " 'transform',\n",
       " 'text',\n",
       " 'data',\n",
       " 'structure',\n",
       " 'format',\n",
       " 'use',\n",
       " 'analysis',\n",
       " 'model',\n",
       " '.',\n",
       " 'process',\n",
       " 'typically',\n",
       " 'involve',\n",
       " 'clean',\n",
       " 'preprocessing',\n",
       " 'text',\n",
       " 'data',\n",
       " ',',\n",
       " 'extract',\n",
       " 'feature',\n",
       " ',',\n",
       " 'apply',\n",
       " 'machine',\n",
       " 'learn',\n",
       " 'algorithm',\n",
       " '.',\n",
       " 'common',\n",
       " 'text',\n",
       " 'analytics',\n",
       " 'task',\n",
       " 'include',\n",
       " 'document',\n",
       " 'classification',\n",
       " ',',\n",
       " 'topic',\n",
       " 'model',\n",
       " ',',\n",
       " 'name',\n",
       " 'entity',\n",
       " 'recognition',\n",
       " ',',\n",
       " 'text',\n",
       " 'summarization',\n",
       " '.',\n",
       " 'increase',\n",
       " 'availability',\n",
       " 'textual',\n",
       " 'data',\n",
       " 'source',\n",
       " 'social',\n",
       " 'medium',\n",
       " ',',\n",
       " 'website',\n",
       " ',',\n",
       " 'document',\n",
       " ',',\n",
       " 'text',\n",
       " 'analytics',\n",
       " 'become',\n",
       " 'essential',\n",
       " 'tool',\n",
       " 'business',\n",
       " ',',\n",
       " 'researcher',\n",
       " ',',\n",
       " 'data',\n",
       " 'scientist',\n",
       " '.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmetized_words = [lemmetizer.lemmatize(word) for word in tokenized_words]\n",
    "\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word, pos='v') if word != ',' else word for word in tokenized_words]\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word, pos='n') if word != ',' else word for word in lemmatized_tokens]  # Nouns\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word, pos='a') if word != ',' else word for word in lemmatized_tokens]  # Adjectives\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word, pos='r') if word != ',' else word for word in lemmatized_tokens]  # Adverbs\n",
    "\n",
    "lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f7a026ba-cfc8-4d8a-aedd-c90c5d10c7d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x91 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 91 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfid_vectorizer = TfidfVectorizer()\n",
    "tfid_matrix = tfid_vectorizer.fit_transform([sample_document])\n",
    "tfid_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2797b1d2-977d-44d6-93d0-0b15d8f21452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.05407381, 0.05407381, 0.10814761, 0.27036904, 0.05407381,\n",
       "        0.43259046, 0.05407381, 0.05407381, 0.10814761, 0.05407381,\n",
       "        0.05407381, 0.05407381, 0.05407381, 0.05407381, 0.10814761,\n",
       "        0.05407381, 0.05407381, 0.27036904, 0.05407381, 0.05407381,\n",
       "        0.05407381, 0.05407381, 0.05407381, 0.05407381, 0.05407381,\n",
       "        0.10814761, 0.05407381, 0.05407381, 0.05407381, 0.05407381,\n",
       "        0.05407381, 0.05407381, 0.05407381, 0.05407381, 0.05407381,\n",
       "        0.05407381, 0.05407381, 0.05407381, 0.10814761, 0.05407381,\n",
       "        0.05407381, 0.05407381, 0.05407381, 0.05407381, 0.05407381,\n",
       "        0.05407381, 0.05407381, 0.10814761, 0.05407381, 0.05407381,\n",
       "        0.05407381, 0.10814761, 0.05407381, 0.10814761, 0.05407381,\n",
       "        0.05407381, 0.05407381, 0.05407381, 0.05407381, 0.05407381,\n",
       "        0.05407381, 0.05407381, 0.05407381, 0.05407381, 0.05407381,\n",
       "        0.05407381, 0.05407381, 0.05407381, 0.05407381, 0.10814761,\n",
       "        0.05407381, 0.05407381, 0.05407381, 0.05407381, 0.54073807,\n",
       "        0.05407381, 0.05407381, 0.16222142, 0.05407381, 0.10814761,\n",
       "        0.05407381, 0.05407381, 0.05407381, 0.05407381, 0.05407381,\n",
       "        0.05407381, 0.10814761, 0.05407381, 0.05407381, 0.05407381,\n",
       "        0.05407381]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfid_df = pd.DataFrame(tfid_matrix.toarray(),columns =tfid_vectorizer.get_feature_names_out())\n",
    "tfid_df\n",
    "tfid_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddef7c5-afc8-4989-8946-d93fae2f3ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
